"""
Download conversational AI model for MedIntel chat interface
"""

import logging
import os
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def download_chat_model():
    """Download the DialoGPT model for conversational AI"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # Create cache directory
        cache_dir = Path("./models/cache")
        cache_dir.mkdir(parents=True, exist_ok=True)

        model_name = "microsoft/DialoGPT-small"

        logger.info(f"üì• Downloading {model_name}...")
        logger.info("This may take a few minutes...")

        # Download tokenizer
        logger.info("‚¨áÔ∏è Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=str(cache_dir))

        # Download model
        logger.info("‚¨áÔ∏è Downloading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, cache_dir=str(cache_dir)
        )

        logger.info("‚úÖ Chat model downloaded successfully!")
        logger.info(f"üìÅ Model cached in: {cache_dir.absolute()}")
        logger.info(
            "\nüéâ Your chat interface is now ready for AI-powered conversations!"
        )

        return True

    except Exception as e:
        logger.error(f"‚ùå Error downloading model: {e}")
        logger.info(
            "\n‚ö†Ô∏è Don't worry! The system will still work using an intelligent fallback."
        )
        logger.info(
            "You can try downloading the model later by running this script again."
        )
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("ü§ñ MedIntel - Chat Model Downloader")
    print("=" * 60)
    print()

    success = download_chat_model()

    print()
    print("=" * 60)
    if success:
        print("‚úÖ Setup complete! You can now use the chat interface.")
    else:
        print("‚ö†Ô∏è Model download incomplete, but fallback system is ready.")
    print("=" * 60)
